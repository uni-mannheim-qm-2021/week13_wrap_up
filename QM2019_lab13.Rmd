---
title: |
  | Wrap-up
author: "Marcel Neunhoeffer & Oliver Rittmann & Denis Cohen"
date: "November 28, 2019 | December 2, 2019 | December 3, 2019"
output:
  html_notebook:
    toc: no
  html_document:
    toc: no
  pdf_document:
    toc: yes
---

---

Today:

1.  Simulation, Simulation... Visualizing Interaction Effects in non-linear models
2.  Simulation, Simulation... A range of values in a count model
3.  Throwback Thursday/Monday/Tuesday

Goals for Today:

*  Get some more practice with simulations.
*  See what you learned this semester

---
```{r setup, echo=FALSE, include=FALSE}
# Load the stuff we need today.

p_needed <-
  c("knitr",
    "MASS",
    "foreign",
    "stargazer",
    "ggplot2"
    )
packages <- rownames(installed.packages())
p_to_install <- p_needed[!(p_needed %in% packages)]
if (length(p_to_install) > 0) {
  install.packages(p_to_install)
}
sapply(p_needed, require, character.only = TRUE)

rm(list = ls())
```



# 1. Simulation, Simulation... Visualizing Interaction Effects in non-linear models

Unfortunately, the intuition about interaction terms form linear models does not extend to non-linear models.

However, we have one really powerful tool in our toolbox that can help us to look at and interpret interactions in any model. Simulation!

As some of you had problems with the interaction effect in a logit model in Homework 9, we will look at an interaction in a logit model here.

The same logic applies to any other non-linear model.

---

For the last time, we will start with some fake data.

```{r}
# Population Size

n <- 100000

# True Parameters

beta0 <- -2
beta1 <- 0.3
beta2 <- 0.5
beta3 <- -0.2

# Independent Variables

X1 <- rnorm(n, 20, 10)
X2 <- rnorm(n, 1, 0.5)

# Our Systematic component

mu <- beta0 + beta1 * X1 + beta2 * X2 + beta3 * X1 * X2

# Now we generate p via the logit response function

p <- (exp(mu)) / (1 + exp(mu))


# As we observe only 0 or 1 we need to put p in a binomial distribution

Y <- rbinom(n, 1, p)

# That's our full population.

pop <- data.frame(Y, X1, X2)

# Let's work with a sample from our population

data <- pop[sample(1:10000, 1000),]


# Now we can run the model...

m1 <-
  glm(Y ~ X1 + X2 + X1 * X2,
      data = data,
      family = binomial(link = logit))

summary(m1)
```

So now we can't make sense of these coefficients... 

# To see what those coefficients mean, we use simulation.

## A. Simulate Parameters - Remember the Steps?

Steps for Simulating Parameters:

* 1. Get the coefficients from the regression (gamma.hat)
* 2. Get the variance-covariance matrix (V.hat)
* 3. Set up a multivariate normal distribution N(gamma.hat,V.hat)
* 4. Draw from the distribution nsim times

```{r}
gamma_hat <- coef(m1)
V_hat <- vcov(m1)

S <- mvrnorm(1000, gamma_hat, V_hat)
```


## B. Calculate Expected Values

Set up interesting scenarios. That's the important step here!

```{r}
X1_sim <- seq(min(X1), max(X1), length.out = 100)

X2_lo <- quantile(X2, 0.25)
X2_hi <- quantile(X2, 0.75)

scenario_X2lo <- cbind(1, X1_sim, X2_lo, X1_sim * X2_lo)
scenario_X2hi <- cbind(1, X1_sim, X2_hi, X1_sim * X2_hi)

Xbeta_lo <- S %*% t(scenario_X2lo)
Xbeta_hi <- S %*% t(scenario_X2hi)

dim(Xbeta_lo)
dim(Xbeta_hi)

# To get expected values for p, we need to plug in the Xbeta values into
# the response function to get simulatd probabilities

p_sim_lo <- (exp(Xbeta_lo)) / (1 + exp(Xbeta_lo))
p_sim_hi <- (exp(Xbeta_hi)) / (1 + exp(Xbeta_hi))

dim(p_sim_lo)
dim(p_sim_hi)


# Means and Quantiles

p_mean_lo <- apply(p_sim_lo, 2, mean)
p_qu_lo <- t(apply(p_sim_lo, 2, quantile, prob = c(0.025, 0.975)))

p_mean_hi <- apply(p_sim_hi, 2, mean)
p_qu_hi <- t(apply(p_sim_hi, 2, quantile, prob = c(0.025, 0.975)))

## C. Plot

plot(
  X1_sim,
  p_mean_lo,
  type = "n",
  ylim = c(0, 1),
  ylab = "Probability of Y",
  xlab = "X1",
  bty = "n",
  las = 1
)

polygon(
  c(rev(X1_sim), X1_sim),
  c(rev(p_qu_lo[, 2]), p_qu_lo[, 1]),
  col = adjustcolor("lightblue", alpha = 0.5),
  border = NA
)

polygon(
  c(rev(X1_sim), X1_sim),
  c(rev(p_qu_hi[, 2]), p_qu_hi[, 1]),
  col = adjustcolor("lightgray", alpha = 0.5),
  border = NA
)


lines(X1_sim, p_mean_lo, lwd = 1)
lines(X1_sim, p_qu_lo[, 1], lty = "dashed", col = "gray20")
lines(X1_sim, p_qu_lo[, 2], lty = "dashed", col = "gray20")


lines(X1_sim, p_mean_hi, lwd = 1)
lines(X1_sim, p_qu_hi[, 1], lty = "dashed", col = "gray20")
lines(X1_sim, p_qu_hi[, 2], lty = "dashed", col = "gray20")


# Add a "histogram" of actual X1-values.

axis(
  1,
  at = X1,
  col.ticks = "gray30",
  labels = FALSE,
  tck = 0.02
) 
```

How about looking at the first difference of the two scenarios directly?

```{r}
fd <- p_sim_lo - p_sim_hi

fd_mean <- apply(fd, 2, mean)

fd_qu <- t(apply(fd, 2, quantile, prob = c(0.025, 0.975)))



plot(
  X1_sim,
  fd_mean,
  type = "n",
  ylim = c(-1, 1),
  ylab = "Difference in predicted probabilities",
  xlab = "X1",
  bty = "n",
  las = 1
)

polygon(
  c(rev(X1_sim), X1_sim),
  c(rev(fd_qu[, 2]), fd_qu[, 1]),
  col = adjustcolor("lightblue", alpha = 0.5),
  border = NA
)

abline(h = 0)
lines(X1_sim, fd_mean, lwd = 1)
lines(X1_sim, fd_qu[, 1], lty = "dashed", col = "gray20")
lines(X1_sim, fd_qu[, 2], lty = "dashed", col = "gray20")


axis(
  1,
  at = X1,
  col.ticks = "gray30",
  labels = FALSE,
  tck = 0.02
) 
```

That looks funky (and not linear at all...).

# 2. Simulation, Simulation... A range of values in a count model

Let's simulate some more. We take another look at the data last week. We make our model 2 from last week slightly more interesting, by adding an interaction between regime type and the number of prior one-sided killings.

```{r}
dta <- read.dta("eck_rep.dta")
#  Some data preparation

dta$os_best[dta$os_best == 500000] <- NA # Rwanda

m2 <-
  glm.nb(
    os_best ~ intensity_dyad + auto + demo + govt + prior_os*auto + prior_os*demo,
    data = dta,
    control = glm.control(maxit = 200)
  )
summary(m2)
```


## A. Simulate Parameters - Remember the Steps?

Steps for Simulating Parameters (Estimation Uncertainty)
1. Get the coefficients from the regression (gamma.hat)
2. Get the variance-covariance matrix (V.hat)
3. Set up a multivariate normal distribution N(gamma.hat,V.hat)
4. Draw from the distribution nsim times


```{r}
nsim <- 1000

gamma_hat <- coef(m2)

V_hat <- vcov(m2)

S <- mvrnorm(nsim, gamma_hat, V_hat)
```


## B. Calculate Expected Values

Set up interesting scenarios.

Autocracies

```{r}
coef(m2)
prior_os_seq <-
  round(seq(min(dta$prior_os, na.rm = TRUE),
      quantile(dta$prior_os, 0.95, na.rm = T),
      length.out = 100), 0)

scenario1 <-
  cbind(
    1,
    median(dta$intensity_dyad, na.rm = TRUE),
    1,
    0,
    median(dta$govt, na.rm = TRUE),
    prior_os_seq,
    prior_os_seq,
    0
  ) 
```


Democracies

```{r}
scenario2 <- cbind(
  1,
  median(dta$intensity_dyad, na.rm = TRUE),
  0,
  1,
  median(dta$govt, na.rm = TRUE),
  prior_os_seq,
  0,
  prior_os_seq
)
```


Anocracies

```{r}
scenario3 <- cbind(
  1,
  median(dta$intensity_dyad, na.rm = TRUE),
  0,
  0,
  median(dta$govt, na.rm = TRUE),
  prior_os_seq,
  0,
  0
) 
```


```{r}
Xbeta1 <- S %*% t(scenario1)

Xbeta2 <- S %*% t(scenario2)

Xbeta3 <- S %*% t(scenario3)
```


To get expected values for lambda, we need to plug in the Xbeta values into the response function 

```{r}
lambda1 <- exp(Xbeta1)

lambda2 <- exp(Xbeta2)

lambda3 <- exp(Xbeta3)
```

Now we need an additional step: Plug the lambda and theta into the negative binomial distribution. And then we average over the fundamental uncertainty for expected values. Please have a look at the King et al. 2001 article from Week 7 again for further information. 

Get the theta.

```{r}
theta <- m2$theta

exp_auto <-
  matrix(sapply(lambda1, function(x)
    mean(rnbinom(1000, size = theta, mu = x))), nrow = 1000)

exp_demo <-
  matrix(sapply(lambda2, function(x)
    mean(rnbinom(1000, size = theta, mu = x))), nrow = 1000)

exp_ano <-
  matrix(sapply(lambda3, function(x)
    mean(rnbinom(1000, size = theta, mu = x))), nrow = 1000)
```


Summarize the results.

```{r}
median_auto <- apply(exp_auto, 2, median)
quants_auto <- t(apply(exp_auto, 2, quantile, c(0.025, 0.975)))

median_demo <- apply(exp_demo, 2, median)
quants_demo <- t(apply(exp_demo, 2, quantile, c(0.025, 0.975)))

median_ano <- apply(exp_ano, 2, median)
quants_ano <- t(apply(exp_ano, 2, quantile, c(0.025, 0.975)))
```

Plot it.

```{r}
plot(
  prior_os_seq,
  median_auto,
  type = "n",
  ylim = c(0, 1000),
  ylab = "Expected Number of one-sided Killings",
  xlab = "Prior one-sided Killings",
  bty = "n",
  las = 1
)

polygon(
  c(rev(prior_os_seq), prior_os_seq),
  c(rev(quants_auto[, 2]), quants_auto[, 1]),
  col = adjustcolor("lightblue", alpha = 0.2),
  border = NA
)

polygon(
  c(rev(prior_os_seq), prior_os_seq),
  c(rev(quants_demo[, 2]), quants_demo[, 1]),
  col = adjustcolor("lightgray", alpha = 0.2),
  border = NA
)

polygon(
  c(rev(prior_os_seq), prior_os_seq),
  c(rev(quants_ano[, 2]), quants_ano[, 1]),
  col = adjustcolor("maroon3", alpha = 0.2),
  border = NA
)

lines(prior_os_seq, median_auto, lwd = 2)
lines(prior_os_seq, quants_auto[, 1], lwd = 0.5, lty = "dashed", col = "lightblue")
lines(prior_os_seq, quants_auto[, 2], lwd = 0.5, lty = "dashed", col = "lightblue")

lines(prior_os_seq, median_demo, lwd = 2, lty = "dotted")
lines(prior_os_seq, quants_demo[, 1], lwd = 0.5, lty = "dashed", col = "lightgray")
lines(prior_os_seq, quants_demo[, 2], lwd = 0.5, lty = "dashed", col = "lightgray")


lines(prior_os_seq, median_ano, lwd = 2, lty = "dashed")
lines(prior_os_seq, quants_ano[, 1], lty = "dashed", col = "maroon3")
lines(prior_os_seq, quants_ano[, 2], lty = "dashed", col = "maroon3")


# Add a "histogram" of actual X1-values.

axis(
  1,
  at = dta$prior_os,
  col.ticks = "gray30",
  labels = FALSE,
  tck = 0.02
) 

legend(
  "topleft",
  legend = c(
    "Autocracy - median",
    "Autocracy - 95% CI",
    "Democracy - median",
    "Democracy - 95% CI",
    "Anocracy - median",
    "Anocracy - 95% CI"
  ),
  col = c(
    "black",
    adjustcolor("lightblue", alpha = 0.5),
    "black",
    adjustcolor("lightgray", alpha = 0.5),
    "black",
     adjustcolor("maroon3", alpha = 0.5)
  ),
  lty = c("solid", NA, "dotted", NA, "dashed", NA),
  lwd = c(2, NA, 2, NA, 2, NA),
  pch = c(NA, 15, NA, 15, NA, 15),
  pt.cex = 2,
  bty = "n"
)
```



# 3. Throwback Thursday/Monday/Tuesday

Remember your first two lab exercises?

## Exercise I:

 - Create three objects:
    1. "my_lucky_number" it should contain your lucky number
    2. "my_firstname" it should contain your firstname
    3. "my_lastname" it should contain your lastname

After you created the objects, call them separately. Don't forget to add comments to your code.

```{r Exercise I}

# We first create the objects

my_lucky_number <- 23

my_firstname <- "Oliver"

my_lastname <- "Rittmann"

# Now we want to call the objects

my_lucky_number
my_firstname
my_lastname

```

##  Exercise II: Selecting and recoding elements
   a) Create two vectors vec1 and vec2
      vec1 should contain 1, 56, 23, 89, -3 and 5 (in that order)
      vec2 contains 24, 78, 32, 27, 8 and 1
   b) Now select elements of vec1 that are greater than 5 or smaller than 0
   c) Next set vec1 to zero if vec2 is greater than 30 and smaller or equal to 32
   
```{r}
# a)

vec1 <- c(1, 56, 23, 89, -3, 5)
vec2 <- c(24, 78, 32, 27, 8, 1)

# b)

vec1[vec1 > 5 | vec1 < 0]

# c)

vec1[vec2 > 30 & vec2 <= 32] <- 0

```


Remember your first homework?

![Homework 1.1.](HW1_1.png)

![Homework 1.2.](HW1_2.png)


It took quite some time back then...

How long do you think would it take you now? 

It's really amazing what you learnt this semester.

So we think you are well prepared to master the Data Essay! 

![Data Essay Marking](DE.png)

